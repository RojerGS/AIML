{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning for the Tower of Hanoi\n",
    "========================\n",
    "\n",
    "Introduction\n",
    "-------------\n",
    "\n",
    "The purpose of this notebook is to be a self-contained explanation of the RL algorithms used in [my GitHub repo](https://github.com/RojerGS/AIML/tree/master/HanoiRL) where I try to solve the Tower of Hanoi.\n",
    "\n",
    "I will try to be as clear as possible in everything I do, as I will also be taking parts of this notebook (as I write them) and publish them in my [maths and programming blog](https://mathspp.blogspot.com).\n",
    "\n",
    "Contents of this notebook\n",
    "---------------------------\n",
    " * [The problem](#theproblem)\n",
    "   - [Encoding the Tower of Hanoi as a markov decision process](#encoding)\n",
    "   - [Visualizing a game session](#visualization)\n",
    " * [Value iteration](#value_iteration)\n",
    " * [Policy iteration](#policy_iteration)\n",
    " * [Q-learning](#q_learning)\n",
    "   - [Double Q-learning](#doubleq_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"theproblem\"></a>\n",
    "The problem\n",
    "------------\n",
    "\n",
    "The Tower of Hanoi is a puzzle in the form of a simple toy:\n",
    "\n",
    "![A Tower of Hanoi toy](https://4.bp.blogspot.com/-RQ2qBzhT4uQ/W7OoofQs0QI/AAAAAAAABDc/Dw5qg_55sGYXEFZ9bIzhUj6TMtVipAvSQCLcBGAs/s1600/frame08.png \"Tower of Hanoi\")\n",
    "\n",
    "composed of three poles and a set of different-sized disks (4 in the image above). The disks start all in the left pole, with the biggest disk on the bottom, all the way up to the smallest disk on top. The goal of the puzzle is to move *all* disks to a different pole than the starting one by moving one disk at a time. There is only one simple restriction: a disk can never be on top of a smaller disk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encoding\"></a>\n",
    "### Encoding the Tower of Hanoi as a markov decision process\n",
    "\n",
    "The first thing to do is to encode the puzzle of the Tower of Hanoi as a markov decision process. Recall that an [mdp](https://mathspp.blogspot.com/2018/09/markov-decision-processes-basics.html) needs five ingredients to be well-defined, a quintuple $(S, A, P_a, R_a, \\gamma)$ with the states, the actions, the transition probabilities, the rewards and the discount factor $\\gamma$. The discount factor will be left undefined for now and we will fiddle with it when we implement the different algorithms.\n",
    "\n",
    "For this explanation assume the number of disks $N$ is fixed. We will define $3^N + 1$ states: $3^N$ are all the ways in which the disks can be distributed legally among the three poles. The $+1$ is for the terminal state we want to reach when the algorithm thinks the puzzle has been completed.\n",
    "\n",
    "For the regular game states we will use a list of three lists, where each sublist will have the numbers corresponding to the disks in that pole; for example, the list\n",
    "\n",
    "```\n",
    "[\n",
    "[],\n",
    "[3,2,1],\n",
    "[4]\n",
    "]\n",
    "```\n",
    "\n",
    "corresponds to the setting of the above image. Notice that bigger numbers correspond to bigger disks and that the bottom of the pole corresponds to the beginning of the list, hence the list above can be further identified with the image above if we rotate the image $90^\\circ$ clockwise; the terminal state will be represented by a constanst.\n",
    "\n",
    "We can thus define the following `define_states` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FINAL_STATE = \"DONE\"    # special state for when the Hanoi Tower was solved\n",
    "\n",
    "def define_states(n):\n",
    "    \"\"\"Recursively define all the possible states of an n-disk Hanoi tower\"\"\"\n",
    "    def aux(n):\n",
    "        if n == 1:\n",
    "            return [\n",
    "                    [[1],[],[]],\n",
    "                    [[],[1],[]],\n",
    "                    [[],[],[1]]\n",
    "                    ]\n",
    "        else:\n",
    "            partial = aux(n-1)\n",
    "            final = []\n",
    "            for state in partial:\n",
    "                a, b, c = state\n",
    "                final.append([[n]+a, b, c])\n",
    "                final.append([a, [n]+b, c])\n",
    "                final.append([a, b, [n]+c])\n",
    "            return final\n",
    "\n",
    "    return [FINAL_STATE] + aux(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the states we now define the actions. We will define $7$ actions: $6$ of them will be the states corresponding to moving disks from one of the poles to any of the other two poles; those will be written as 2-character strings. The first character of the string represents the pole from which we removed and the second character represents the pole in which we want to insert.\n",
    "\n",
    "The seventh action is for when the algorithm thinks it has completed the puzzle and hence \"claims\" the puzzle has been finished. For this action we use another constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COMPLETED = \"COMPLETED\" # special action when the MDP thinks we are done\n",
    "actions = [COMPLETED, \"LC\", \"LR\", \"CL\", \"CR\", \"RL\", \"RC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having defined states and actions we are now in position to define transition probabilities and rewards. We will assume all actions are available in all game states (_except_ for the terminal state where no action is available) but the restrictions imposed by the rules will be reflected in the transitions.\n",
    "\n",
    "Whenever a move is illegal (goes against the rules), whenever the move tries to remove a disk from an empty pole or the algorithm claims the puzzle is complete but isn't, we incurr in a penalty and we transition to the exact same state.\n",
    "\n",
    "Whenever the action represents a move and is legal, the state transition occurs as per the puzzle rules and there is no reward (i.e. the reward is $0$).\n",
    "\n",
    "Whenever we claim the puzzle is complete from one of the two final positions and thus enter the terminal state, we get a positive reward.\n",
    "\n",
    "This information is all encoded in the function `transition` which also makes use of the helper function `game_is_done`, used to check if the puzzle has indeed been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def game_is_done(state):\n",
    "    \"\"\"Return if the Hanoi Tower was solved;\n",
    "        Assume we always start with all disks on the left\"\"\"\n",
    "    return (len(state[0]) == len(state[1]) == 0 and \\\n",
    "                sorted(state[2], reverse=True) == state[2]) or \\\n",
    "            (len(state[0]) == len(state[2]) == 0 and \\\n",
    "                sorted(state[1], reverse=True) == state[1])\n",
    "\n",
    "def transition(state, action):\n",
    "    \"\"\"Given a <state> and an <action>, return the new state\n",
    "        and the reward we got.\n",
    "    The actions are of the form L|C|R + L|C|R,\n",
    "        where the first character says from where we remove (Left,Centre,Right)\n",
    "        the second character says where we are inserting (Left,Centre,Right)\n",
    "        OR\n",
    "        COMPLETED to transition into the final state when the game is done\"\"\"\n",
    "    R_illegal = -3      # reward if the action was an illegal move\n",
    "    R_final = 5         # reward if we end the game\n",
    "    R_default = 0       # default reward\n",
    "\n",
    "    # deal with the action COMPLETED separately\n",
    "    if action == COMPLETED:\n",
    "        if game_is_done(state):\n",
    "            return FINAL_STATE, R_final\n",
    "        else:\n",
    "            return state, R_illegal\n",
    "\n",
    "    state_ = deepcopy(state)\n",
    "    # disassemble the state and the action\n",
    "    d = {'L': 0, 'C': 1, 'R': 2}\n",
    "    i = d[action[0]]    # remove from this pile\n",
    "    j = d[action[1]]    # insert into this pile\n",
    "    if not state_[i]:   # can we remove?\n",
    "        return state, R_illegal\n",
    "    moving = state_[i].pop()\n",
    "    if (state_[j] and moving > state_[j][-1]):\n",
    "        # illegal move found\n",
    "        return state, R_illegal\n",
    "    else:\n",
    "        state_[j].append(moving)\n",
    "        return state_, R_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define an auxiliary function `dictify` that turns a game state into something that can be used as a dictionary key (because lists can't). I didn't know this would be useful from the start, but turns out that when we use the algorithms it kind of helps, so lets just take that out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictify(state):\n",
    "    \"\"\"Turns a state into a valid dict key\"\"\"\n",
    "    if state == FINAL_STATE:\n",
    "        return state\n",
    "    else:\n",
    "        return tuple([tuple(item) for item in state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, we recursively defined all the states in `define_states(N)` and implemented a transition function `transition(state, action)` that handles the transitions and the rewards. We also implemented two helper functions, `game_is_done)state)` to recognize if a state corresponds to one where the game is done and `dictify(state)` that will come in handy when we deal with dictionaries. We also defined all the available actions.\n",
    "\n",
    "Below I present all the code as I have it in my `hanoi_mdp.py` file, available [in my GitHub](https://github.com/RojerGS/AIML/tree/master/HanoiRL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# contents of hanoi_mdp.py, available at https://github.com/RojerGS/AIML/tree/master/HanoiRL\n",
    "from copy import deepcopy\n",
    "\n",
    "COMPLETED = \"COMPLETED\" # special action when the MDP thinks we are done\n",
    "FINAL_STATE = \"DONE\"    # special state for when the Hanoi Tower was solved\n",
    "actions = [COMPLETED, \"LC\", \"LR\", \"CL\", \"CR\", \"RL\", \"RC\"]\n",
    "\n",
    "def dictify(state):\n",
    "    \"\"\"Turns a state into a valid dict key\"\"\"\n",
    "    if state == FINAL_STATE:\n",
    "        return state\n",
    "    else:\n",
    "        return tuple([tuple(item) for item in state])\n",
    "\n",
    "def define_states(n):\n",
    "    \"\"\"Recursively define all the possible states of an n-disk Hanoi tower\"\"\"\n",
    "    def aux(n):\n",
    "        if n == 1:\n",
    "            return [\n",
    "                    [[1],[],[]],\n",
    "                    [[],[1],[]],\n",
    "                    [[],[],[1]]\n",
    "                    ]\n",
    "        else:\n",
    "            partial = aux(n-1)\n",
    "            final = []\n",
    "            for state in partial:\n",
    "                a, b, c = state\n",
    "                final.append([[n]+a, b, c])\n",
    "                final.append([a, [n]+b, c])\n",
    "                final.append([a, b, [n]+c])\n",
    "            return final\n",
    "\n",
    "    return [FINAL_STATE] + aux(n)\n",
    "\n",
    "def game_is_done(state):\n",
    "    \"\"\"Return if the Hanoi Tower was solved;\n",
    "        Assume we always start with all disks on the left\"\"\"\n",
    "    return (len(state[0]) == len(state[1]) == 0 and \\\n",
    "                sorted(state[2], reverse=True) == state[2]) or \\\n",
    "            (len(state[0]) == len(state[2]) == 0 and \\\n",
    "                sorted(state[1], reverse=True) == state[1])\n",
    "\n",
    "def transition(state, action):\n",
    "    \"\"\"Given a <state> and an <action>, return the new state\n",
    "        and the reward we got.\n",
    "    The actions are of the form L|C|R + L|C|R,\n",
    "        where the first character says from where we remove (Left,Centre,Right)\n",
    "        the second character says where we are inserting (Left,Centre,Right)\n",
    "        OR\n",
    "        COMPLETED to transition into the final state when the game is done\"\"\"\n",
    "    R_illegal = -3      # reward if the action was an illegal move\n",
    "    R_final = 5         # reward if we end the game\n",
    "    R_default = 0       # default reward\n",
    "\n",
    "    # deal with the action COMPLETED separately\n",
    "    if action == COMPLETED:\n",
    "        if game_is_done(state):\n",
    "            return FINAL_STATE, R_final\n",
    "        else:\n",
    "            return state, R_illegal\n",
    "\n",
    "    state_ = deepcopy(state)\n",
    "    # disassemble the state and the action\n",
    "    d = {'L': 0, 'C': 1, 'R': 2}\n",
    "    i = d[action[0]]    # remove from this pile\n",
    "    j = d[action[1]]    # insert into this pile\n",
    "    if not state_[i]:   # can we remove?\n",
    "        return state, R_illegal\n",
    "    moving = state_[i].pop()\n",
    "    if (state_[j] and moving > state_[j][-1]):\n",
    "        # illegal move found\n",
    "        return state, R_illegal\n",
    "    else:\n",
    "        state_[j].append(moving)\n",
    "        return state_, R_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a game session\n",
    "<a id=\"visualization\"></a>\n",
    "\n",
    "The function `print_game_sequence` defined below gets as input a list with game states and prints a frame for each state, for us to visualize the game progression.\n",
    "\n",
    "This function uses `matplotlib.pyplot` so make sure you have that installed to use this \"feature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as mp\n",
    "\n",
    "def print_game_sequence(game):\n",
    "    \"\"\"Given a Tower of Hanoi game sequence,\n",
    "        print a sequence of images representing the game\"\"\"\n",
    "    if not game:\n",
    "        return\n",
    "    \n",
    "    default_colours = [\"b\",\"g\",\"r\",\"c\",\"m\",\"y\"] # default colours\n",
    "    N = sum(map(len, game[0]))\n",
    "    if N <= len(default_colours):\n",
    "        colours = default_colours[:N]\n",
    "    else:\n",
    "        colours = [[i/N]*3 for i in range(N)] # use a scale of grays\n",
    "    \n",
    "    values = [i for i in range(N,0,-1)]\n",
    "    for frame in game:\n",
    "        # start with 3 tiny bars to represent the poles\n",
    "        xs = [2,4,6]\n",
    "        hs = [N]*3\n",
    "        ws = [0.1]*3\n",
    "        cs = [[0]*3]*3\n",
    "        for bar in frame:\n",
    "            xs += [2*(1+frame.index(bar))]*len(bar)\n",
    "            for idx in range(len(bar)-1,-1,-1):\n",
    "                hs.append(idx+1)\n",
    "                w = 0.2 + 1.6*(bar[idx]/N)\n",
    "                ws.append(w)\n",
    "                cs.append(colours[bar[idx]-1])\n",
    "        # add three \"bars\" that together make the base\n",
    "        xs += [2,4,6]\n",
    "        hs += [0.1]*3\n",
    "        ws += [2]*3\n",
    "        cs += [[0.2, 0.8, 0.66]]*3\n",
    "        mp.bar(xs, hs, ws, color = cs)\n",
    "        mp.axis(\"off\")\n",
    "        mp.ylim(0, N)\n",
    "        mp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA/NJREFUeJzt3MFNw0AQQNEsoiPohXIQ5dAL1DQ0YKEcQjb4v9dAZjPW\n117sNTMXADqedg8AwH0JP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APEPO8e4D9a63Ka151nLmv3\nDI9urXW475nx3210tBc7uY4bP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzw\nA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPAD\nxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APE\nCD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QI\nP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/\nQIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9A\njPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8Ssmdk9\nw59bH+v8h3wQ8z5r9wy3ttbx8zNzvrP+J0d7sZPruPEDxAg/QIzwA8QIP0CM8APECD9AjPADxAg/\nQIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9A\njPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM\n8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzw\nA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPAD\nxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APE\nCD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxKyZ2T3DTb1+f57rQEDG18vbusfvuPEDxAg/QIzw\nA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0DM6T7ZAMDv3PgBYoQfIEb4AWKEHyBG+AFihB8gRvgB\nYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFi\nfgBfsCjrH9XMRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4d5b9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA/NJREFUeJzt3MFNxDAUANEY0RH0QjmIcugFavocOKFFHBCsIfNeA/mJ\n45EvyZqZA4COm90DAHBdwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMbe7B+CjtdbFp9Qzs3bM\nwrvP1uQ4rMtu9sr3OfEDxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPAD\nxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APE\nCD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QI\nP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/\nQIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9A\njPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM\n8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AzJqZ3TP8uvW0\nzn+Tf8Q8ztp17bWOU6zzzLHtGf4na13u65l9799/4sQPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8\nADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wA\nMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAx\nwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHC\nDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIP\nECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8Q\nI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPELNmZvcMP+r+9flcNwRkvNw9rGtcx4kfIEb4AWKE\nHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFiTvfLBgC+5sQPECP8ADHCDxAj/AAxwg8QI/wAMcIP\nECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8Q\n8wYm5yjrqC2ExQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA/FJREFUeJzt3MFNxDAUANEY0RH0QjmIcugFavocuIAWcUCwFpn3GsiP\nbI98SdbMHAB03OweAIDrEn6AGOEHiBF+gBjhB4gRfoAY4QeIEX6AGOEHiLndPQCfrbUuPqWembVj\nFt59tSbHYV12c1Z+zo0fIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQf\nIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8g\nRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG\n+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4\nAWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgB\nYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFi\nhB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYtbM7J6BD9Za\nFwsyM2vHLP/NWscpNvPMsW2919Pl/uNvzOO+c+3GDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAx\nwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHC\nDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIP\nECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8Q\nI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj\n/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8\nADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxCzZmb3DL/q/vX5XC8EZLzcPaxrPMeNHyBG+AFihB8g\nRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYk73ywYAvufGDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj\n/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPEPMG\nJuco671DrF8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8271860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA/VJREFUeJzt3MFJBEEQQNFuMSPNxXDEcMxFYyoPXpRdxMOyA/PfS2Cq\nqeHTl5k9MwuAjoejBwDgvoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWIejx6A3/beF59Sz8w+\nYha+XdvJWvbyH3uv0/waYGadZt9u/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8\nADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wA\nMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAx\nwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHC\nDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIP\nECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8Q\nI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxAj/AAxwg8QI/wAMcIPECP8ADHCDxCz\nZ+boGfhh732xkJnZR8zCt2s7Weuce9lv18/K7c3rce+PGz9AjPADxAg/QIzwA8QIP0CM8APECD9A\njPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM\n8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzw\nA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPAD\nxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APE\nCD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8QI\nP0CM8APECD9AjPADxAg/QIzwA8QIP0CM8APECD9AzJ6Zo2e4qefP93MdCMj4eHrZ93iOGz9AjPAD\nxAg/QIzwA8QIP0CM8APECD9AjPADxAg/QIzwA8Sc7pcNAPzNjR8gRvgBYoQfIEb4AWKEHyBG+AFi\nhB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKEHyBG+AFihB8gRvgBYoQfIEb4AWKE\nHyDmC4vdKOspj9XCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8315a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example usage of print_game_sequence with a game of 2 disks played optimally\n",
    "game = []\n",
    "game.append([[2,1],[],[]])\n",
    "game.append([[2],[1],[]])\n",
    "game.append([[],[1],[2]])\n",
    "game.append([[],[],[2,1]])\n",
    "print_game_sequence(game)\n",
    "del(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"value_iteration\"></a>\n",
    "Value iteration\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"policy_iteration\"></a>\n",
    "Policy iteration\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"q_learning\"></a>\n",
    "Q-learning\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"doubleq_learning\"></a>\n",
    "### Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
